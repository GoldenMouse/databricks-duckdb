{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duckdb on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demos how to use Duckdb on databricks.\n",
    "It shows three common ways to load tables on Databricks into duckdb process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0343087b-864a-4726-a9a4-c1538b9c7e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Installation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the extensions via the tarball avoids having to do LOAD command later\n",
    "- httpfs: for loading files from S3\n",
    "- delta: for loading tables using delta table files (my preferred method)\n",
    "- unity_catalog: for loading tables directly from unity catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f79d08-5687-4627-bb2a-b37f24aefefe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "DUCK_VER=\"v1.4.3\"\n",
    "ARCH=\"linux_amd64\"\n",
    "BASE_EXT_URL=\"https://extensions.duckdb.org/${DUCK_VER}/${ARCH}\"\n",
    "EXT_PATH=\"${HOME}/.duckdb/extensions/${DUCK_VER}/${ARCH}\"\n",
    "\n",
    "pip install  --upgrade duckdb[all]==${DUCK_VER}\n",
    "\n",
    "EXTENSIONS=(\"httpfs\" \"delta\" \"unity_catalog\") \n",
    "\n",
    "# 4. Download and decompress each extension\n",
    "mkdir -p ${EXT_PATH}\n",
    "for EXT in \"${EXTENSIONS[@]}\"; do\n",
    "  echo \"Installing DuckDB extension: ${EXT}...\"\n",
    "  wget -q -O \"${EXT_PATH}/${EXT}.duckdb_extension.gz\" \"${BASE_EXT_URL}/${EXT}.duckdb_extension.gz\"\n",
    "  gzip -d -f \"${EXT_PATH}/${EXT}.duckdb_extension.gz\"\n",
    "  echo \"Successfully installed ${EXT}.\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5ed198a-42b2-4446-b9cb-caaad589cef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading Table into DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31015101-5e7d-4e82-bbd4-f62f7dbe215e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Without any extensions - Using Spark intermediary table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85639c33-ca9c-453e-90d4-c2e5d624cb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is the easiest method which doesn't require any extensions.\n",
    "However, it's slower with the added overhead of converting the tables first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba531fe3-1360-4559-91e4-4412506116ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using spark and converting to arrow (or pandas)\n",
    "import duckdb\n",
    "\n",
    "TABLE_NAME = \"catalog.myschema.customers\"\n",
    "\n",
    "spark_df = spark.sql(f\"SELECT * FROM {TABLE_NAME} LIMIT 5000000\")\n",
    "customers = spark_df.toArrow() # or toPandas()\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee6bb12-b50b-4522-a309-20de4a72e2d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = con.sql(\"SELECT * FROM customers\").to_df()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d94d87-c15a-44e1-a8b9-0be430d190ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac9945f-079d-4d35-910f-41939cb8732b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Unity Catalog extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b4e2549-5cdc-4280-97e1-8e39883d8aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "UC Catalog extension is in preview and not recommended for production workloads\n",
    "Using this extension, DuckDB is able to access UC Catalogs directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad5d771-34c2-4c0b-9e66-331340fb3f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# connect to UC\n",
    "import duckdb\n",
    "region = \"us-west-1\"\n",
    "raw_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "workspace_url = f\"https://{raw_url}\"\n",
    "print(workspace_url)\n",
    "\n",
    "# we use the secret from the notebook session\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"LOAD unity_catalog\")\n",
    "print(con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE SECRET (\n",
    "        TYPE unity_catalog,\n",
    "        TOKEN '{token}',\n",
    "        ENDPOINT '{workspace_url}',\n",
    "        AWS_REGION '{region}'\n",
    "    );\n",
    "\"\"\").df())\n",
    "\n",
    "catalog_name = \"catalog\"\n",
    "con.execute(f\"ATTACH OR REPLACE '{catalog_name}' AS unity (TYPE UNITY_CATALOG);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c30ba4de-4a06-4d2d-a628-bb1daff54c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create duckdb table\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE customers AS \n",
    "    SELECT * \n",
    "    FROM unity.tf_test.customers \n",
    "    LIMIT 5000000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcde6e9-7181-40c3-a737-77e0e08494b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Casting required because unity_catalog extension does not correctly map the high-precision DECIMAL(38, x) data type from Unity Catalog\n",
    "# compute agg\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    customer_id,\n",
    "    MIN(CAST(amount AS DOUBLE)) AS min_amount,\n",
    "    MAX(CAST(amount AS DOUBLE)) AS max_amount,\n",
    "    AVG(CAST(amount AS DOUBLE)) AS avg_amount,\n",
    "    SUM(CAST(amount AS DOUBLE)) AS total_amount\n",
    "FROM customers\n",
    "GROUP BY\n",
    "    1\n",
    "\"\"\"\n",
    "\n",
    "print(\"Building aggregate table\")\n",
    "customers_agg = con.sql(query)\n",
    "df = con.sql(\"SELECT * FROM customers_agg\").df()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3671ae-5032-4e34-b20d-95a523a55108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5c6c8d5-7256-4fd2-97bb-c77e1953753e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using S3/Delta extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f0b3c2-1dcc-4951-ade9-62f6566c94db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using this extension, DuckDB is able to access the Delta table files in S3 directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43ef687-5f1f-4b06-bbcf-9bb83630fcd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import TableOperation\n",
    "\n",
    "table_name = \"catalog.myschema.customers\"\n",
    "region = \"us-west-1\"\n",
    "db_path = \"/Workspace/Users/username/customers.ddb\"\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "#  Fetch Temporary Credentials from Unity Catalog\n",
    "#  This requires 'EXTERNAL USE SCHEMA' permission on the schema\n",
    "table_info = w.tables.get(table_name)\n",
    "creds = w.temporary_table_credentials.generate_temporary_table_credentials(\n",
    "    table_id=table_info.table_id,\n",
    "    operation=TableOperation.READ\n",
    ").aws_temp_credentials\n",
    "\n",
    "# Configure DuckDB Secret with the Temporary Session Token\n",
    "print(f\"Connecting to duckdb {db_path}\")\n",
    "\n",
    "# Here we persistenting the database by point to a file in the Workspace. Useful when the table is too large for memory\n",
    "con = duckdb.connect(database=db_path, read_only=False)\n",
    "\n",
    "print(\"Getting duckdb secret\")\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE SECRET (\n",
    "        TYPE S3,\n",
    "        PROVIDER config,\n",
    "        KEY_ID '{creds.access_key_id}',\n",
    "        SECRET '{creds.secret_access_key}',\n",
    "        SESSION_TOKEN '{creds.session_token}',\n",
    "        REGION '{region}'\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Run delta_scan using the authenticated session\n",
    "storage_loc = table_info.storage_location\n",
    "print(f\"Scanning: {storage_loc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ffbb20-a5a6-4f00-8590-6c489baf8442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as a DuckDB table\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE customers_view AS\n",
    "    SELECT * FROM delta_scan('{storage_loc}') LIMIT 5000000;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed5e73d-8f8f-4f05-9fd3-fe31a954bf56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate the duckdb table\n",
    "agg_table_name = \"catalog.myschema.customers_agg\"\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    customer_id,\n",
    "    min(amount) AS min_amount,\n",
    "    max(amount) AS max_amount,\n",
    "    avg(amount) AS avg_amount,\n",
    "    sum(amount) AS total_amount\n",
    "FROM customers_view\n",
    "GROUP BY\n",
    "    1\n",
    "\"\"\"\n",
    "\n",
    "print(\"Building aggregate table\")\n",
    "adf = con.execute(query).fetch_arrow_table()\n",
    "\n",
    "print(\"Writing to Delta Table \")\n",
    "sdf = spark.createDataFrame(adf)\n",
    "sdf.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(agg_table_name)\n",
    "\n",
    "display(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029edfac-5e80-4a0d-9b88-aa3a8a057329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4372de41-e997-4043-8a08-9df210afd1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Benchmarking: Spark and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3252778-3404-4381-b987-ea03634bcb24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The below code is for comparision to see how fast DuckDB is compare to traditional spark with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1863e6d8-9317-4820-b379-92cff25a1d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TABLE_NAME = \"catalog.myschema.customers\"\n",
    "spark_df = spark.sql(f\"SELECT * FROM {TABLE_NAME} LIMIT 5000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c01e64d-b4bd-4ad3-82ed-ce4e2ccf6fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark_df.toPandas().groupby(\"customer_id\").agg(\n",
    "    max_amount=pd.NamedAgg(column=\"amount\", aggfunc=\"max\"),\n",
    "    min_amount=pd.NamedAgg(column=\"amount\", aggfunc=\"min\"),\n",
    "    avg_amount=pd.NamedAgg(column=\"amount\", aggfunc=\"mean\"),\n",
    "    total_amount=pd.NamedAgg(column=\"amount\", aggfunc=\"sum\")\n",
    ").reset_index()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d79ae6-c3b7-4c6a-9218-93f26f2f6b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min, max, avg, sum\n",
    "df = spark_df.groupBy(\"customer_id\").agg(\n",
    "    min(col(\"amount\")).alias(\"min_amount\"),\n",
    "    avg(col(\"amount\")).alias(\"avg_amount\"),\n",
    "    max(col(\"amount\")).alias(\"max_amount\"),\n",
    "    sum(col(\"amount\")).alias(\"total_amount\")\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "STANDARD"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "duckdb[all]==1.4.3"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8053181751098077,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "New 2026 test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
